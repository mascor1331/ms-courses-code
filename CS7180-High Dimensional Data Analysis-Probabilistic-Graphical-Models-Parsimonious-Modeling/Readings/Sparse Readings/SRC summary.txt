In this paper, we exploit the discriminative nature of sparse representation to perform classification. Instead of using the generic dictionaries discussed above, we represent the test sample in an overcomplete dictionary whose base elements are the training samples themselves. If sufficient training samples are available from each class, 2 it will be possible to represent the test samples as a linear combina- tion of just those training samples from the same class. This representation is naturally sparse, involving only a small fraction of the overall training database. We argue that in many problems of interest, it is actually the sparsest linear representation of the test sample in terms of this dictionary and can be recovered efficiently via L1-minimization. Seeking the sparsest representation therefore automatically discriminates between the various classes present in the training set. Fig. 1 illustrates this simple idea using face recognition as an example. Sparse representation also provides a simple and surprisingly effective means of rejecting invalid test samples not arising from any class in the training database: these samples’ sparsest representations tend to involve many dictionary elements, spanning multiple classes.

For ease of presentation, we shall first assume that the training samples from a single class do lie on a subspace. This is the only prior knowledge about the training samples we will be using in our solution.

Notice,
though, that using the entire training set to solve for x
represents a significant departure from one sample or one
class at a time methods such as NN and NS. We will later
argue that one can obtain a more discriminative classifier
from such a global representation. We will demonstrate its
superiority over these local methods (NN or NS) both for
identifying objects represented in the training set and for
rejecting outlying samples that do not arise from any of the
classes present in the training set. These advantages can
come without an increase in the order of growth of the
computation: As we will see, the complexity remains linear
in the size of training set.

However, the problem of finding the sparsest
solution of an underdetermined system of linear equations is
NP-hard and difficult even to approximate [13]: that is, in the
general case, no known procedure for finding the sparsest
solution is significantly more efficient than exhausting all
subsets of the entries for x.

------------------------------------------------------------------------------
INTRODUCTION
In this paper, the authors approach the problems like relevance of efficient feature extraction and robustness to corruption, occlusion, and disguise in face detection domain from the perspective of sparse representations achieving accuracy on par with current top of the class algorithms like nearest neighbour, nearest subspace and support vector machines. The key insight behind that is the given the training data is sufficiently sparse, it can directly be used to express test data as a combination of its elements borrowing from its applications in compressesion sensing and signal recovery.

ALGORITHM
Taking the only assumption of representing samples from a single class in low dimensional linear subspace under various lighting conditions often found to capture most of variations in data, therefore a sufficient number of different classes and its resulting combination could be used to represent the incoming image which even though might not always be unique but can be normalized and instead taking it a step further  for classification by finding the sparsest form of above representation from the same class of samples minimizing computation cost using L2 norm and L1 minimization. Authors also compare the ability of L1 or L2 to lead towards sparse solution finding L1 less dense and hence more informative. Validation accounts for joint distribution of all classes to clearly answer whether the test image is too noisy/generic or not a face present in training set at all which is also demonstrated to be working empirically.


PROBLEM
The authors then address the two fundamental issues in face recognition firstly choice of feature transformation on which most of the algorithms are dependent. This is a curse and boon at the same time since dimensionality reduction (Eigenfaces, Fisherfaces) brings with it bad and good both features in compressed representation. Authors proposed the linear projection generated by Gaussian random matrix - Randomfaces being independent of training dataset and efficient to generate are found to be equally good classification result as long as the correct sparse solution is recovered. Second problem they tackle is robustness to occlusion or corruption of images where they remind readers that no bases or features are better than original image which had far more pixels than subjects themselves and compression should be avoid unless computational resources are limited.


EXPERIMENTS
The SRC algorithm is tested on randomly sampled (50-50 train-test split) publically available datasets in comparision with NN, NS and linear SVM compared unbiased way minimizing limitations of all algorithms compared to SRC in three domains from feature extraction, robustness, and classification. SRC and SVM perform better than NN and NS on chosen data that cover most aspects of frontal face detection. Another limitation that does not hold SRC back is the strong correlation of accuracy with optimal feature selection where the author's proposed randomfaces came out better in terms of dimensions and computability. The next test that SRC outshined rest comprised of detecting faces from partial face features even with high dimensional features. SRC recovers the original image with 99.3% accuracy even in cases the original image is pixel corrupted randomly till 60% whereas the best of rest (linear SVM) was only able to recover 73% at 50% corruption. Author also remind that properly computing redundancy and sparsity is critical to recovery of image and show due that SRC is able to recover image 90.3% at 40% random block occlusion. Recognition despite disguise is dependent on neighborliness of training set and is sensitive to disguises that resemble regions in another image from different samples. 

IMPROVEMENTS
Improved recognition can be obtained by partitioning test and train image in blocks and representing the test image blocks holistically as sparse representation of train image blocks. Seeking the sparsest representation also provides a simple and surprisingly effective means of rejecting invalid test samples not arising from any class in the training database since these samples’ sparsest representations tend to involve many dictionary elements, spanning multiple classes. The authors provide for a way empirically to design training set for robustness and maximized accuracy by having samples with maximal variation per class. 


CONCLUSIONS & FUTURE
With theoretical reasoning and superior experimental results, authors proved that coming up with optimal sparse representation is highly rewarding for face recognition even in presence of corruption and occlusion and leave it on future works to explore its applications in the other domains such as object detection. Finally ending on a critical note addressing one assumption made of reduction of image in linear subspace, the sufficient number of samples for non linear distributions would turn out large and other being lighting variation extrapolation in this case would require computationally advanced techniques which could be explored further in future work.

